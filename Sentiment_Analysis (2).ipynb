{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "04993594-3941-4e5f-bee4-3aa205e14bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shreeyapandey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0d73274a-40ae-48d5-91d1-ae3fad750f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "data = pd.read_csv('imdb_data.csv')\n",
    "\n",
    "#Data Pr-processing\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    return text.lower()\n",
    "\n",
    "data['review'] = data['review'].apply(clean_text)\n",
    "\n",
    "# Tokenization and removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "data['tokens'] = data['review'].apply(tokenize_and_remove_stopwords)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "data['stemmed_tokens'] = data['tokens'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
    "\n",
    "# Convert text to sequences\n",
    "tokenizer = Tokenizer(num_words=8000)\n",
    "tokenizer.fit_on_texts(data['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
    "sequences = tokenizer.texts_to_sequences(data['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
    "text_lengths = [min(len(seq), 200) for seq in sequences]  # Ensure all sequences are <= 200\n",
    "sequences = [seq[:200] for seq in sequences]  # Truncate sequences\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=200, padding='post', truncating='post')\n",
    "y = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test, lengths_train, lengths_test = train_test_split(X, y, text_lengths, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, y_train, lengths_train = map(torch.tensor, (X_train, y_train, lengths_train))\n",
    "X_test, y_test, lengths_test = map(torch.tensor, (X_test, y_test, lengths_test))\n",
    "\n",
    "# Create TensorDatasets and DataLoaders\n",
    "train_data = TensorDataset(X_train, y_train, lengths_train)\n",
    "test_data = TensorDataset(X_test, y_test, lengths_test)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b70e449d-e78a-48bf-83e4-bdfe2b7d4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Design\n",
    "class SentimentBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional=True, drop_prob=0.5):\n",
    "        super(SentimentBiLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Consider using pre-trained embeddings here\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, text_lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, _) = self.lstm(packed_embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.sig(self.fc(hidden))\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SentimentBiLSTM(8000, 400, 256, 1, 2, True, 0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added L2 regularization\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "97077d39-5628-4c17-a982-8b47287b0bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4717\n",
      "Epoch 2, Loss: 0.3022\n",
      "Epoch 3, Loss: 0.2486\n",
      "Epoch 4, Loss: 0.2126\n",
      "Epoch 5, Loss: 0.1772\n",
      "Epoch 6, Loss: 0.0997\n",
      "Epoch 7, Loss: 0.0729\n",
      "Epoch 8, Loss: 0.0557\n",
      "Epoch 9, Loss: 0.0419\n",
      "Epoch 10, Loss: 0.0320\n",
      "Epoch 11, Loss: 0.0230\n",
      "Epoch 12, Loss: 0.0211\n",
      "Epoch 13, Loss: 0.0195\n",
      "Epoch 14, Loss: 0.0184\n",
      "Epoch 15, Loss: 0.0174\n"
     ]
    }
   ],
   "source": [
    "#Training of the model\n",
    "def train(model, train_loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths_batch).squeeze()\n",
    "        y_batch = y_batch.float()  \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()  # Update the learning rate\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "num_epochs = 15  # Increased epochs\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, scheduler)\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33f0eb-e7e3-47cc-a920-713e4fbb9b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8c1958bd-450a-43bc-8349-debf979e30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8759\n",
      "Precision: 0.8706\n",
      "Recall: 0.8853\n",
      "F1 Score: 0.8779\n",
      "Confusion Matrix:\n",
      "[[4298  663]\n",
      " [ 578 4461]]\n",
      "AUC Score: 0.9310\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "    predicted_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths_batch in test_loader:\n",
    "            outputs = model(X_batch, lengths_batch).squeeze()\n",
    "            probs = outputs.numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            true_labels.extend(y_batch.numpy())\n",
    "            predictions.extend(preds)\n",
    "            predicted_probs.extend(probs)  # Store probabilities for AUC calculation\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    auc_score = roc_auc_score(true_labels, predicted_probs)  # Corrected use of predicted probabilities\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7e3a6-f41d-4301-8320-ed6d40baf618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
